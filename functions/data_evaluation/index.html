
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Model Evaluation - TDC</title>
    <link rel="stylesheet" href="/TDC/assets/css/app.css">
    <link rel="shortcut icon" type="image/png"
           href="/TDC/favicon.png" 
    />
    <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Model Evaluation | TDC</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Model Evaluation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Datasets for Medicinal Machine Learning" />
<meta property="og:description" content="Datasets for Medicinal Machine Learning" />
<link rel="canonical" href="http://localhost:4000/TDC/functions/data_evaluation/" />
<meta property="og:url" content="http://localhost:4000/TDC/functions/data_evaluation/" />
<meta property="og:site_name" content="TDC" />
<script type="application/ld+json">
{"url":"http://localhost:4000/TDC/functions/data_evaluation/","headline":"Model Evaluation","description":"Datasets for Medicinal Machine Learning","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<!-- head scripts --></head>

  <body>
    
<nav class="navbar is-primary" >
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-brand" href="/TDC/">
            <span><img src="/TDC/logonav.png" alt="Logo" style="height: auto; width: auto; max-height: 45px; max-width: 250px;"></span>
            </a>
            <a role="button" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu">
            <div class="navbar-end">
                
                
                    
                    <a href="/TDC/start/" class="navbar-item ">Quick-Start</a>
                    
                
                    
                    <div class="navbar-item has-dropdown is-hoverable">
                        <a href="/TDC/overview/" class="navbar-link ">Datasets</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/TDC/overview/" class="navbar-item ">Overview</a>
                            
                            <a href="/TDC/single_pred_tasks/overview" class="navbar-item ">Single-instance Prediction</a>
                            
                            <a href="/TDC/multi_pred_tasks/overview" class="navbar-item ">Multi-instance Prediction</a>
                            
                            <a href="/TDC/generation_tasks/overview" class="navbar-item ">Generation</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <div class="navbar-item has-dropdown is-hoverable">
                        <a href="/TDC/fct_overview/" class="navbar-link ">Data Functions</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/TDC/fct_overview/" class="navbar-item ">Overview</a>
                            
                            <a href="/TDC/functions/data_evaluation/" class="navbar-item is-active">Model Evaluation</a>
                            
                            <a href="/TDC/functions/data_process/" class="navbar-item ">Data Processing</a>
                            
                            <a href="/TDC/functions/data_split/" class="navbar-item ">Data Split</a>
                            
                            <a href="/TDC/functions/oracles/" class="navbar-item ">Molecule Generation Oracles</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <a href="/TDC/team/" class="navbar-item ">Team</a>
                    
                
                <a href="https://github.com/mims-harvard/TDC" class="navbar-item">GitHub</a>
                
            </div>

        </div>
    </div>
</nav>

    
    


    <section class="section">
        <div class="container">
            <div class="columns">
                
                <div class="column is-4-desktop is-4-tablet">
                    

<aside class="menu">

    <p class="menu-label"></p>
    <ul class="menu-list">
        
        <li>
            <a href="/TDC/#" class=""><strong>Data Functions</strong></a>
            
            <ul>
                
                
                <li><a href="/TDC/fct_overview/" class="">Overview</a></li>
                
                
                
                <li><a href="/TDC/functions/data_evaluation/" class="is-active">Model Evaluation</a></li>
                
                
                
                <li><a href="/TDC/functions/data_process/" class="">Data Processing</a></li>
                
                
                
                <li><a href="/TDC/functions/data_split/" class="">Data Split</a></li>
                
                
                
                <li><a href="/TDC/functions/oracles/" class="">Molecule Generation Oracles</a></li>
                
                
            </ul>
            
        </li>
            
    </ul>

</aside>
                </div>
                
                <div class="column is-8">
                    
                    
                    
                    
    
    

<div class="contents">
    <div class="menu">
        <p class="menu-label">Function Index</p>
        <ul class="menu-list">
  <li><a href="#regression-metric">Regression Metric</a>
    <ul>
      <li><a href="#mean-squared-error-mse">Mean Squared Error (MSE)</a></li>
      <li><a href="#mean-absolute-error-mae">Mean Absolute Error (MAE)</a></li>
      <li><a href="#coefficient-of-determination-r">Coefficient of Determination (R²)</a></li>
    </ul>
  </li>
  <li><a href="#binary-classification-metric">Binary Classification Metric</a>
    <ul>
      <li><a href="#area-under-the-receiver-operating-characteristic-curve-roc-auc">Area Under the Receiver Operating Characteristic Curve (ROC-AUC)</a></li>
      <li><a href="#area-under-the-precision-recall-curve-pr-auc">Area Under the Precision-Recall Curve (PR-AUC)</a></li>
      <li><a href="#accuracy-metric">Accuracy Metric</a></li>
      <li><a href="#precision">Precision</a></li>
      <li><a href="#recall">Recall</a></li>
      <li><a href="#f1-score">F1 Score</a></li>
    </ul>
  </li>
  <li><a href="#multi-class-classification-metric">Multi-class Classification Metric</a>
    <ul>
      <li><a href="#micro-f1-micro-precision-micro-recall-accuracy">Micro-F1, Micro-Precision, Micro-Recall, Accuracy</a></li>
      <li><a href="#macro-f1">Macro-F1</a></li>
      <li><a href="#cohens-kappa-kappa">Cohen’s Kappa (Kappa)</a></li>
    </ul>
  </li>
  <li><a href="#token-level-classification-metric">Token-level Classification Metric</a>
    <ul>
      <li><a href="#average-roc-auc">Average ROC-AUC</a></li>
    </ul>
  </li>
</ul>
    </div>
</div>




<div class="content">
    <h2 id="regression-metric">Regression Metric</h2>

<h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3>

<p class="is-size-6">  <strong> Description: </strong> The mean square error measures the averages of the squares of the errors between the true value and the predicted value. It takes in a list/array of true values <code>y_true</code> and a list/array of predicted values <code>y_pred</code>, and outputs the MSE value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'MSE'</span><span class="p">)</span>
<span class="c1"># y_true: [0.8, 0.7, ...]; y_pred: [0.75, 0.73, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.1
</span></code></pre></div></div>

<h3 id="mean-absolute-error-mae">Mean Absolute Error (MAE)</h3>
<p class="is-size-6">  <strong> Description: </strong> The mean absolute error measures the averages of the absolute value of the errors between the true value and the predicted value. It takes in a list/array of true values <code>y_true</code> and a list/array of predicted values <code>y_pred</code>, and outputs the MAE value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'MAE'</span><span class="p">)</span>
<span class="c1"># y_true: [0.8, 0.7, ...]; y_pred: [0.75, 0.73, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.1
</span></code></pre></div></div>

<h3 id="coefficient-of-determination-r">Coefficient of Determination (R²)</h3>

<p class="is-size-6">  <strong> Description: </strong> The R² measures the amount of associations between the true values and the predicted values. It takes in a list/array of true values <code>y_true</code> and a list/array of predicted values <code>y_pred</code>, and outputs the R² score.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'R2'</span><span class="p">)</span>
<span class="c1"># y_true: [0.8, 0.7, ...]; y_pred: [0.75, 0.73, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.8
</span></code></pre></div></div>

<h2 id="binary-classification-metric">Binary Classification Metric</h2>

<h3 id="area-under-the-receiver-operating-characteristic-curve-roc-auc">Area Under the Receiver Operating Characteristic Curve (ROC-AUC)</h3>

<p class="is-size-6">  <strong> Description: </strong> The ROC-AUC measures the area under the ROC curve which is plotting the true positive and false positive at various threshold. It takes in a list/array of binary true values <code>y_true</code> and a list/array of real-valued predicted scores <code>y_pred</code>, and outputs the ROC-AUC score.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'ROC-AUC'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 0, ...]; y_pred: [0.75, 0.23, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.88
</span></code></pre></div></div>

<h3 id="area-under-the-precision-recall-curve-pr-auc">Area Under the Precision-Recall Curve (PR-AUC)</h3>

<p class="is-size-6">  <strong> Description: </strong> The PR-AUC measures the area under the precision-recall curve which is plotting the precision and recall at various threshold. It takes in a list/array of binary true values <code>y_true</code> and a list/array of real-valued predicted scores <code>y_pred</code>, and outputs the PR-AUC score.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'PR-AUC'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 0, ...]; y_pred: [0.75, 0.23, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.88
</span></code></pre></div></div>

<h3 id="accuracy-metric">Accuracy Metric</h3>

<p class="is-size-6">  <strong> Description: </strong> The accuracy calculates the fraction of correct prediction at a threshold. It takes in a list/array of binary true values <code>y_true</code> and a list/array of real-valued predicted scores <code>y_pred</code> and a threshold value <code>thr</code>, and outputs the accuracy score. The default of threshold value is 0.5. </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'Accuracy'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 0, ...]; y_pred: [0.75, 0.23, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># score: 0.8
</span></code></pre></div></div>

<h3 id="precision">Precision</h3>

<p class="is-size-6">  <strong> Description: </strong> The precision calculates the fraction of correctly predicted positive instance out of the total predicted positive values at a threshold. It takes in a list/array of binary true values <code>y_true</code> and a list/array of real-valued predicted scores <code>y_pred</code> and a threshold value <code>thr</code>, and outputs the precision score. The default of threshold value is 0.5. </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'Precision'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 0, ...]; y_pred: [0.75, 0.23, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># score: 0.8
</span></code></pre></div></div>

<h3 id="recall">Recall</h3>

<p class="is-size-6">  <strong> Description: </strong> The recall calculates the fraction of correctly predicted positive instance out of all the positive instances at a threshold. It takes in a list/array of binary true values <code>y_true</code> and a list/array of real-valued predicted scores <code>y_pred</code> and a threshold value <code>thr</code>, and outputs the recall score. The default of threshold value is 0.5. </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'Recall'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 0, ...]; y_pred: [0.75, 0.23, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># score: 0.8
</span></code></pre></div></div>

<h3 id="f1-score">F1 Score</h3>

<p class="is-size-6">  <strong> Description: </strong> The F1 is the harmonic mean of recall and precision. It takes in a list/array of binary true values <code>y_true</code> and a list/array of real-valued predicted scores <code>y_pred</code> and a threshold value <code>thr</code>, and outputs the recall score. The default of threshold value is 0.5. </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'F1'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 0, ...]; y_pred: [0.75, 0.23, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># score: 0.8
</span></code></pre></div></div>

<h2 id="multi-class-classification-metric">Multi-class Classification Metric</h2>

<h3 id="micro-f1-micro-precision-micro-recall-accuracy">Micro-F1, Micro-Precision, Micro-Recall, Accuracy</h3>
<p class="is-size-6">  <strong> Description: </strong> The micro-F1 in multi-class prediction is the same as micro-precision, micro-recall and accuracy. It calculates the fraction of correct prediction. It takes in a list/array of true integer label index <code>y_true</code> and a list/array of predicted integer label index <code>y_pred</code>, and outputs the score. </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'MicroF1'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 3, ...]; y_pred: [1, 2, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.8
</span></code></pre></div></div>

<h3 id="macro-f1">Macro-F1</h3>
<p class="is-size-6">  <strong> Description: </strong> The macro-F1 calculates the fraction of correct prediction for each label and then takes the unweighted average. This is useful when the label distribution is highly imbalanced and one wants to test the performance on low-data labels. It takes in a list/array of true integer label index <code>y_true</code> and a list/array of predicted integer label index <code>y_pred</code>, and outputs the score. </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'MacroF1'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 3, ...]; y_pred: [1, 2, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.5
</span></code></pre></div></div>

<h3 id="cohens-kappa-kappa">Cohen’s Kappa (Kappa)</h3>

<p class="is-size-6">  <strong> Description: </strong> The Kappa score calculates the level of agreement between the prediction and the true labels. It takes in a list/array of true integer label index <code>y_true</code> and a list/array of predicted integer label index <code>y_pred</code>, and outputs the Kappa score. </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'Kappa'</span><span class="p">)</span>
<span class="c1"># y_true: [1, 3, ...]; y_pred: [1, 2, ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.5
</span></code></pre></div></div>

<h2 id="token-level-classification-metric">Token-level Classification Metric</h2>

<h3 id="average-roc-auc">Average ROC-AUC</h3>

<p class="is-size-6">  <strong> Description: </strong> The averages ROC-AUC first calculates ROC-AUC score between the sequence of 1/0 true labels and the sequence of prediction labels for every instance. Then, it takes the average of all the instances' ROC-AUC scores. It takes in a list of list/array of true integer label index for every instance <code>y_true</code> and a list of list/array of predicted integer label index for every instance <code>y_pred</code>, and outputs the average ROC-AUC score. </p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tdc</span> <span class="kn">import</span> <span class="n">Evaluator</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s">'Avg-ROC-AUC'</span><span class="p">)</span>
<span class="c1"># y_true: [[0, 1, ...], [1, 1, ...], ...]; y_pred: [[0.1, 0.8, ...], [0.9, 0.89, ...], ...]
</span><span class="n">score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1"># score: 0.8
</span></code></pre></div></div>

</div>
                </div>
                
            </div>
        </div>
    </section>
    
        <footer class="footer">
    <div class="container">
        
        <div class="columns is-mobile">
            <div class="column is-8 has-text-left is-vcentered">
                <a class="navbar-brand" href="/TDC/">
                    <span><img src="/TDC/tdc_horizontal.png" alt="Logo" style="max-height: 40px; max-width: 250px;"></span>
                </a>
            </div>
            <div class="column is-4 has-text-right is-vcentered">
                <a href="https://github.com/mims-harvard/TDC">
                    <span class="icon is-large">
                      <i class="fas fab fa-github fa-3x"></i>
                    </span>
                </a>
            </div>
        </div>
        
    </div>
</footer>
    
    <script src="/TDC/assets/js/app.js" type="text/javascript"></script><!-- footer scripts --></body>
</html>

